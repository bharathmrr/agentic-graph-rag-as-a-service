#!/usr/bin/env python3
"""
Enhanced LLM Setup Script for Agentic Graph RAG
Installs Gemini, Groq, and Ollama dependencies with progress tracking
"""

import subprocess
import sys
import os
import asyncio
from pathlib import Path

def run_command(command, description=""):
    """Run a command and handle errors"""
    print(f"ğŸ”„ {description}")
    try:
        result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)
        print(f"âœ… {description} - Success")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ {description} - Failed: {e.stderr}")
        return False

def install_dependencies():
    """Install all LLM dependencies"""
    print("ğŸš€ Setting up Enhanced LLM Integration for Agentic Graph RAG")
    
    # Core dependencies
    dependencies = [
        # Gemini dependencies
        "google-generativeai>=0.3.0",
        "langchain-google-genai>=1.0.0",
        
        # Groq dependencies (already installed)
        "groq>=0.4.0",
        "llama-index-llms-groq>=0.1.0",
        
        # Ollama dependencies
        "langchain-community>=0.0.20",
        "ollama>=0.1.7",
        
        # Enhanced processing
        "asyncio-mqtt>=0.13.0",
        "aiofiles>=23.2.0",
        
        # Additional ML libraries
        "transformers>=4.35.0",
        "sentence-transformers>=2.2.2",
        "torch>=2.0.0",
        "spacy>=3.7.0",
        "numpy>=1.24.0",
        "scikit-learn>=1.3.0"
    ]
    
    print(f"ğŸ“¦ Installing {len(dependencies)} packages...")
    
    for dep in dependencies:
        success = run_command(f"pip install {dep}", f"Installing {dep}")
        if not success:
            print(f"âš ï¸  Failed to install {dep}, continuing...")
    
    # Download spaCy model
    run_command("python -m spacy download en_core_web_sm", "Downloading spaCy English model")
    
    print("âœ… Enhanced LLM dependencies installed!")

def create_env_template():
    """Create environment template for API keys"""
    env_template = """
# Enhanced LLM Configuration
# Add your API keys here

# Google Gemini API Key (Required for Gemini)
GOOGLE_API_KEY=your_google_api_key_here

# Groq API Key (Required for Groq)
GROQ_API_KEY=your_groq_api_key_here

# Optional: Model configurations
GEMINI_MODEL=gemini-2.0-flash
GROQ_MODEL=llama3-8b-8192
OLLAMA_MODEL=llama3.2:latest

# Processing settings
CHUNK_SIZE=512
CHUNK_OVERLAP=20
SIMILARITY_THRESHOLD=0.7
"""
    
    env_file = Path(".env.enhanced")
    if not env_file.exists():
        with open(env_file, "w") as f:
            f.write(env_template.strip())
        print(f"ğŸ“ Created {env_file} template - Please add your API keys")
    else:
        print(f"ğŸ“„ {env_file} already exists")

def check_ollama_installation():
    """Check if Ollama is installed and running"""
    print("ğŸ” Checking Ollama installation...")
    
    try:
        result = subprocess.run("ollama --version", shell=True, capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… Ollama is installed")
            
            # Check if Ollama service is running
            try:
                result = subprocess.run("ollama list", shell=True, capture_output=True, text=True)
                if result.returncode == 0:
                    print("âœ… Ollama service is running")
                    
                    # Check for llama3.2 model
                    if "llama3.2" in result.stdout:
                        print("âœ… Llama 3.2 model is available")
                    else:
                        print("ğŸ“¥ Downloading Llama 3.2 model (this may take a while)...")
                        run_command("ollama pull llama3.2:latest", "Downloading Llama 3.2")
                else:
                    print("âš ï¸  Ollama service not running. Please start it with: ollama serve")
            except:
                print("âš ï¸  Could not check Ollama service status")
        else:
            print("âŒ Ollama not found. Please install from: https://ollama.ai/")
            print("   After installation, run: ollama pull llama3.2:latest")
    except:
        print("âŒ Ollama not found. Please install from: https://ollama.ai/")

def test_integrations():
    """Test all LLM integrations"""
    print("ğŸ§ª Testing LLM integrations...")
    
    test_script = """
import os
import asyncio

# Test environment variables
print("ğŸ”§ Environment Variables:")
print(f"  GOOGLE_API_KEY: {'âœ… Set' if os.getenv('GOOGLE_API_KEY') else 'âŒ Not set'}")
print(f"  GROQ_API_KEY: {'âœ… Set' if os.getenv('GROQ_API_KEY') else 'âŒ Not set'}")

# Test imports
try:
    import google.generativeai as genai
    from langchain_google_genai import ChatGoogleGenerativeAI
    print("âœ… Gemini imports successful")
except ImportError as e:
    print(f"âŒ Gemini imports failed: {e}")

try:
    from groq import Groq
    print("âœ… Groq imports successful")
except ImportError as e:
    print(f"âŒ Groq imports failed: {e}")

try:
    from langchain_community.embeddings import OllamaEmbeddings
    import ollama
    print("âœ… Ollama imports successful")
except ImportError as e:
    print(f"âŒ Ollama imports failed: {e}")

try:
    from backend.services.enhanced_llm_service import EnhancedLLMService, create_gemini_service
    from backend.services.file_processing_pipeline import FileProcessingPipeline
    print("âœ… Enhanced services imports successful")
except ImportError as e:
    print(f"âŒ Enhanced services imports failed: {e}")

print("ğŸ‰ Import tests completed!")
"""
    
    try:
        exec(test_script)
    except Exception as e:
        print(f"âŒ Test failed: {e}")

def create_demo_script():
    """Create a demo script to test the integration"""
    demo_script = '''#!/usr/bin/env python3
"""
Demo script for Enhanced LLM Integration
Tests Gemini, Groq, and Ollama functionality
"""

import os
import asyncio
from backend.services.enhanced_llm_service import create_gemini_service, create_groq_service
from backend.services.file_processing_pipeline import FileProcessingPipeline

async def demo_gemini():
    """Demo Gemini functionality"""
    print("\\nğŸ¤– Testing Gemini Integration:")
    try:
        service = create_gemini_service()
        
        text = "Apple Inc. was founded by Steve Jobs in Cupertino, California."
        result = await service.extract_entities(text)
        
        if result["success"]:
            print(f"âœ… Found {len(result['entities'])} entities")
            for entity in result["entities"][:3]:
                print(f"  - {entity.get('text', 'N/A')} ({entity.get('type', 'N/A')})")
        else:
            print(f"âŒ Failed: {result.get('error', 'Unknown error')}")
            
    except Exception as e:
        print(f"âŒ Gemini test failed: {e}")

async def demo_groq():
    """Demo Groq functionality"""
    print("\\nâš¡ Testing Groq Integration:")
    try:
        service = create_groq_service()
        
        result = await service.chat_response("What is artificial intelligence?")
        
        if result["success"]:
            print(f"âœ… Response: {result['response'][:100]}...")
        else:
            print(f"âŒ Failed: {result.get('error', 'Unknown error')}")
            
    except Exception as e:
        print(f"âŒ Groq test failed: {e}")

async def demo_file_processing():
    """Demo file processing pipeline"""
    print("\\nğŸ“„ Testing File Processing Pipeline:")
    try:
        pipeline = FileProcessingPipeline()
        
        sample_text = """
        Artificial Intelligence (AI) is transforming industries worldwide. 
        Companies like Google, Microsoft, and OpenAI are leading the development 
        of advanced AI systems. Machine learning and deep learning are key 
        technologies driving this revolution.
        """
        
        job_id = await pipeline.process_file("demo.txt", sample_text)
        print(f"âœ… Started processing job: {job_id}")
        
        # Wait a bit and check status
        await asyncio.sleep(2)
        status = pipeline.get_job_status(job_id)
        if status:
            print(f"ğŸ“Š Job status: {status['status']} ({status['overall_progress']}%)")
        
    except Exception as e:
        print(f"âŒ File processing test failed: {e}")

async def main():
    """Run all demos"""
    print("ğŸš€ Enhanced LLM Integration Demo")
    print("=" * 50)
    
    await demo_gemini()
    await demo_groq()
    await demo_file_processing()
    
    print("\\nğŸ‰ Demo completed!")
    print("\\nğŸ“‹ Next steps:")
    print("1. Add your API keys to .env.enhanced")
    print("2. Start the backend: python src/api/main.py")
    print("3. Start the frontend: cd frontend && npm start")
    print("4. Navigate to 'Enhanced File Processor' in the dashboard")

if __name__ == "__main__":
    asyncio.run(main())
'''
    
    demo_file = Path("demo_enhanced_llm.py")
    with open(demo_file, "w") as f:
        f.write(demo_script)
    print(f"ğŸ“ Created {demo_file} - Run this to test your setup")

def main():
    """Main setup function"""
    print("=" * 70)
    print("ğŸš€ ENHANCED LLM INTEGRATION SETUP FOR AGENTIC GRAPH RAG")
    print("=" * 70)
    
    # Install dependencies
    install_dependencies()
    
    # Create environment template
    create_env_template()
    
    # Check Ollama
    check_ollama_installation()
    
    # Test integrations
    test_integrations()
    
    # Create demo script
    create_demo_script()
    
    print("\n" + "=" * 70)
    print("ğŸ‰ SETUP COMPLETE!")
    print("=" * 70)
    print("\nğŸ“‹ Next steps:")
    print("1. ğŸ”‘ Get API Keys:")
    print("   - Gemini: https://makersuite.google.com/app/apikey")
    print("   - Groq: https://console.groq.com/")
    print("2. ğŸ“ Add keys to .env.enhanced file")
    print("3. ğŸ‹ Install Ollama: https://ollama.ai/")
    print("4. ğŸ“¥ Pull Llama model: ollama pull llama3.2:latest")
    print("5. ğŸš€ Start backend: python src/api/main.py")
    print("6. ğŸ¨ Start frontend: cd frontend && npm start")
    print("7. ğŸ§ª Test with: python demo_enhanced_llm.py")
    print("\nğŸ¯ Features Available:")
    print("- âš¡ Lightning-fast Groq inference")
    print("- ğŸ¤– Advanced Gemini 2.0 Flash")
    print("- ğŸ”¢ Local Ollama embeddings")
    print("- ğŸ“Š Real-time progress tracking (1-100%)")
    print("- ğŸ”„ Complete file processing pipeline")
    print("- ğŸ¨ Beautiful UI with animated progress")

if __name__ == "__main__":
    main()
